{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Basic Sentiment Analysis on Movie Reviews Database Using Naive Bayes #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#read the data tekonized\n",
    "data_dir = os.getcwd() + \"/datasets/movie_reviews/\" #get the data directory\n",
    "posids = os.listdir(data_dir+\"pos/\")\n",
    "negids = os.listdir(data_dir+\"neg/\")\n",
    "df1 = pd.DataFrame({'Ids' : [x + y for x, y in zip([\"pos/\"]*len(posids), posids)], 'Sentiment' : 1, 'Data' : ''})\n",
    "df2 = pd.DataFrame({'Ids' : [x + y for x, y in zip([\"neg/\"]*len(negids), negids)], 'Sentiment' : -1})\n",
    "data = pd.concat([df1,df2])\n",
    "data = data.reset_index()\n",
    "df1 = None; df2 = None\n",
    "for i in data.index:\n",
    "    f = open(data_dir + data.Ids[i])\n",
    "    text = f.read()\n",
    "    text = nltk.word_tokenize(text) #tokenize the data\n",
    "    data.set_value(i, 'Data', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def remove_from_list(the_list, val):\n",
    "   return [value.lower() for value in the_list if value != val] #remove val from the list as well as lowercase all the strings\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):      #get the equivalent of pos tags in wordnet to use with the wordnet lemmatizer\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#convert words like 're to are and n't to not\n",
    "#remove things like (, ), -, ', \" and :\n",
    "#lowercase everything\n",
    "#get the figures of speech, lemmatize based on it\n",
    "\n",
    "for ind in data.index:\n",
    "    text = data.Data[ind]\n",
    "    symbols = ['(', ')', '-', '\\'', '\\\"', ':', '``']\n",
    "    for symbol in symbols: #remove the aforementioned symbols\n",
    "           text = remove_from_list(text, symbol)\n",
    "\n",
    "    postags = nltk.pos_tag(text)\n",
    "\n",
    "    for i in range(0, len(text)):\n",
    "        if text[i] == \"n't\":\n",
    "            text[i] = 'not'\n",
    "        elif postags[i][1].startswith('V'): #if it's a verb\n",
    "            if text[i] == \"'s\":\n",
    "                text[i] = 'is'\n",
    "            elif text[i] == \"'re\":\n",
    "                text[i] = 'are'\n",
    "            elif text[i] == \"'ve\":\n",
    "                text[i] = 'have'\n",
    "\n",
    "    for i in range(0, len(text)):  #lemmatize\n",
    "        lmtzr = WordNetLemmatizer()\n",
    "        text[i] = lmtzr.lemmatize(text[i], get_wordnet_pos(postags[i][1]))\n",
    "    \n",
    "#     temp = -1\n",
    "#     for i in range(0, len(text)):  #deal with not\n",
    "#         if text[i] == 'not':\n",
    "#             temp = i;\n",
    "#         while temp > 0:\n",
    "#             if (temp < len(text)-1):\n",
    "#                 if (text[temp+1] != ',' and text[temp+1] != '.' and text[temp+1] != ';' and text[temp+1] != '?'):\n",
    "#                     text[temp+1] = 'NOT_' + text[temp+1]\n",
    "#                     temp = temp+1\n",
    "#                 else:\n",
    "#                     temp = -1\n",
    "#             else:\n",
    "#                 temp = -1\n",
    "    \n",
    "    data.set_value(ind, 'Data', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#construct the features vector\n",
    "\n",
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "def word_feats(words):\n",
    "    return dict([(word, True) for word in words])\n",
    " \n",
    "feats = [(word_feats(data.Data[ind]), data.Sentiment[ind]) for ind in data.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on 1500 instances, test on 500 instances\n"
     ]
    }
   ],
   "source": [
    "#divide the data into training set and test set\n",
    "cutoff = len(feats)*3/8\n",
    "#np.random.shuffle(feats)\n",
    "\n",
    "trainfeats = feats[:cutoff] + feats[1000:1000+cutoff]\n",
    "testfeats = feats[cutoff:1000] + feats[1000+cutoff:]\n",
    "print 'train on %d instances, test on %d instances' % (len(trainfeats), len(testfeats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: 97.73\n",
      "test accuracy: 70.6\n"
     ]
    }
   ],
   "source": [
    "#apply the Naive Bayes Classifier\n",
    "classifier = NaiveBayesClassifier.train(trainfeats)\n",
    "print 'training accuracy:', round(nltk.classify.util.accuracy(classifier, trainfeats)*100,2)\n",
    "print 'test accuracy:', round(nltk.classify.util.accuracy(classifier, testfeats)*100,2)\n",
    "#classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
